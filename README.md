# DependencyParsing
Here I build an arc-standard shift-reduce neural dependency parser based on 'A Fast and Accurate Dependency Parser using neural networks' (Chen and Manning, 2014) using CoNLL dataset. Follow the steps below to run the dependency parser.

-----------------------------------------------------
Preparing dataset in the format to be used for training.
-----------------------------------------------------
> preparedata.py

• -i <inputfile> to specify a training file to be read in CoNLL format.
• -o <outputfile> to specify an output file to be used later by train.py.
• -v <vocabfile> to specify a file path to store training vocabulary, default is 'train.vocab'.

The output from preparedata.py will have the features associated with all the parser configurations paired with parser decisions of each training sentence after removing non-projective sentences.
The format of the file is as follows:
* For each configuration (separated by a new line) all the features will be space separated and the parser actions will be tab separated.
* <null> -> empty element in stack or buffer
* <root> -> root element of stack
* <p>text -> POS tag of a word
* <l>label -> arc-label of a word
* The order of the features is as follows : 
Top 3 words of the stack(<null> fills empty places), top 3 words of buffer(<null> fills empty places), leftmost_child of top word of stack, rightmost_child of top word of stack, 
2nd_leftmost_child of top word of stack, 2nd_rightmost_child of top word of stack, leftmost_grandchild of top word of stack, rightmost_grandchild of top word of stack,
, leftmost_child of top word of buffer, rightmost_child of top word of buffer, 2nd_leftmost_child of top word of buffer, 2nd_rightmost_child of top word of buffer, 
leftmost_grandchild of top word of buffer, rightmost_grandchild of top word of buffer, POS tags of all the words added to features prefixed with <p>, 
arc-labels of all the words added to features except top 3 words of stack and buffer.
Total 48 features.

-----------------------------------------------------
Training the neural network model.
-----------------------------------------------------
> train.py

• -u <integer> to specify the number of hidden units.
• -l <float> to specify the learning rate.
• -E <integer> to specify the embedding dimension length
• -b <integer> to specify the batch size.
• -e <integer> to specify the number of epochs to train for.
• -d <float> to specify the dropout value.
• -i <inputfile> to specify the output file generated by preparedata.py.
• -o <modelfile> to specify a model file to be written.
• -v <vocabfile> to specify file path to the vocab file generated by preparedata.py, default; 'train.vocab'.

The output of train.py will be a model file that will be used in parse.py for predictions.

-----------------------------------------------------
Testing - parsing the input file.
-----------------------------------------------------
> parse.py

• -m <modelfile> to specify the trained model file to read.
• -i <inputfile> to specify a test file to be read in CoNLL format.
• -o <outfile> to specify an output file to be written containing parse trees for input.
• -v <vocabfile> to specify the model vocab file to be read, default : 'train.vocab'.

The output of the parse.py will be a file in the CoNLL format having parents and arc-label values filled for each word in the input file.
